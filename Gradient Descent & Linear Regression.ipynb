{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a1aec4f9dcdef63792b63bc36e616a2bd217321"
   },
   "source": [
    "### Linear Regression using Gradient Descent\n",
    "Victor Ernoult\n",
    "<br>\n",
    "\n",
    "Here we implement the Gradient Descent optimization process to find the best coefficients for m & b in a linear regression context. We compare it to the results we find by fitting a least square regression.\n",
    "\n",
    "The data used is taken from one of Kaggle's competition, in which participants are asked to predict house prices based on their features. We will fit a linear regression between the liveable area of a house and its price. \n",
    "\n",
    "Kaggle Link: https://www.kaggle.com/c/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1515.463699</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>525.480383</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>334.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1129.500000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1464.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1776.750000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5642.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         GrLivArea      SalePrice\n",
       "count  1460.000000    1460.000000\n",
       "mean   1515.463699  180921.195890\n",
       "std     525.480383   79442.502883\n",
       "min     334.000000   34900.000000\n",
       "25%    1129.500000  129975.000000\n",
       "50%    1464.000000  163000.000000\n",
       "75%    1776.750000  214000.000000\n",
       "max    5642.000000  755000.000000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "train = train[[\"GrLivArea\", \"SalePrice\"]]\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "756c420d7b1787d2d1968af5af19a3643487fb8f"
   },
   "source": [
    "### OLS Regression\n",
    "\n",
    "We first fit a traditional least squares linear regression to compare the coefficients and the error with the results of the gradient descent optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "85529ea72e87343ab1c70c0a4ac5fe77c3bf7de4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m:  107.13035896582517 b:  18569.02585648728\n",
      "MAE : 37638.72898759625\n"
     ]
    }
   ],
   "source": [
    "# Fitting a Linear Regression the normal way\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "X, y = (np.array(train.iloc[:,0]).reshape(-1, 1), train.iloc[:,1])\n",
    "lr.fit(X, y)\n",
    "\n",
    "print('m: ', lr.coef_[0], 'b: ',lr.intercept_)\n",
    "print(\"MAE :\", mean_absolute_error(y, lr.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fdd623870a975d5c44c0d61cb2beb37a017a99f7"
   },
   "source": [
    "### Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "01afd1f86833a32334ff0a456671d246b57bf7bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m : 118.06882468668596 b : 0.4678990795538888\n",
      "MAE : 38382.12418660609\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(data, iters, learning_rate, display_progress = False):\n",
    "    \"\"\" Assumes a DataFrame will be fed in, with 2 columns, x & y in that order.\"\"\"\n",
    "    m_curr = b_curr = 0\n",
    "    n = len(data)\n",
    "    x = data.iloc[:,0]\n",
    "    y = data.iloc[:,1]\n",
    "    \n",
    "    for i in range(iters):\n",
    "        y_pred = m_curr*x + b_curr\n",
    "        \n",
    "        # Partial derivatives of m & b\n",
    "        md = -(2/n) * sum(x*(y-y_pred))\n",
    "        bd = -(2/n) * sum(y-y_pred)\n",
    "        \n",
    "        # Adjusting the coefficients with derivatives (subject to the learning rate)\n",
    "        m_curr = m_curr - learning_rate * md\n",
    "        b_curr = b_curr - learning_rate * bd\n",
    "        \n",
    "        mae = np.mean(abs(y - (m_curr*x + b_curr)))\n",
    "        if display_progress:\n",
    "            print(\"Iteration\", i, \":\", \"m =\", m_curr, \"// b =\",b_curr, \"// MAE =\", mae)\n",
    "        \n",
    "    return (m_curr, b_curr, mae)\n",
    "\n",
    "m, b, mae = gradient_descent(train, 1000, 0.0000001, False)\n",
    "print(\"m :\", m, \"b :\", b)\n",
    "print(\"MAE :\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ff5a204cabe1e276c5e9c0c7e4aae64bfed0f3c"
   },
   "source": [
    "The coefficients found through this method differ from the least square regression. However, we find a similar error, hinting that the method has not fully converged yet. It is interesting to note that though m quickly finds its optimal value and keeps it, b keeps increasing, but very slowly. This is surely due to the low learning rate, which was necessary to allow convergence on this particular dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c87ffe3119c6a91876fefb4c89eda52eb1c35b1f"
   },
   "source": [
    "### Comparison on simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "2a82cc5cf9b5223f360f44a1ebe1581529bfd903",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Square Regression\n",
      "m:  20.00302941500719 b:  2.596154219244454\n",
      "MAE : 1.182739819267736\n",
      "\n",
      "Gradient Descent\n",
      "m : 20.037440795010948 b : 0.3137090537794922\n",
      "MAE : 1.4331081988278935\n"
     ]
    }
   ],
   "source": [
    "# Comparison of the 2 approaches with simulated data.\n",
    "from random import uniform\n",
    "size = 100\n",
    "simple_data_y = [x*20+uniform(0, 5) for x in range(size)]\n",
    "simple_data_x = np.array(range(size)).reshape(-1, 1)\n",
    "\n",
    "# Least Square\n",
    "print(\"Least Square Regression\")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(simple_data_x, simple_data_y)\n",
    "print('m: ', lr.coef_[0], 'b: ',lr.intercept_)\n",
    "print(\"MAE :\", mean_absolute_error(simple_data_y, lr.predict(simple_data_x)))\n",
    "\n",
    "# Gradient Descent\n",
    "print(\"\\nGradient Descent\")\n",
    "data = pd.DataFrame(data = {\"x\" : range(size), \"y\" : simple_data_y})\n",
    "m, b, mae = gradient_descent(data, 1000, 0.00001, False)\n",
    "print(\"m :\", m, \"b :\", b)\n",
    "print(\"MAE :\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d23f09b5dcbbe03afa358546437f7f87a9f88769"
   },
   "source": [
    "Same conclusion here, gradient descent's results come rather close to OLS, both in terms of coefficients & error. As expected, OLS remains superior and much more time-effective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
